2024-11-12 06:29:43,643 - INFO - Starting.
2024-11-12 06:29:43,644 - INFO - Loading Data.
2024-11-12 06:52:32,444 - INFO - Data loaded in 0.0 hours 22.0 minutes.
2024-11-12 06:52:32,774 - INFO - Training Started.
Epoch 1/5000:   0%|          | 0/26 [00:00<?, ?it/s]Epoch 1/5000:   0%|          | 0/26 [00:33<?, ?it/s]
Traceback (most recent call last):
  File "train.py", line 141, in <module>
    train_model(logger, trainer, train_loader, val_loader, num_epochs=config["num_epochs"], device=config["device"], test_interval=config["test_interval"],save_interval=config["save_interval"], save_dir=save_dir, model_save_path=config["model_save_path"])
  File "train.py", line 39, in train_model
    loss_dict = trainer.train_step(states, obs, reward, goal)
  File "/home/cike/marl_collector/adhoc_dt/Trainer.py", line 82, in train_step
    total_loss, mie_loss, mse_loss_r, mse_loss_g = self.compute_loss(s, o, r_true, g_true)
  File "/home/cike/marl_collector/adhoc_dt/Trainer.py", line 62, in compute_loss
    mie_loss, q_z = self.MIE_loss(self.teammateencoder, self.adhocencoder, self.beta, self.gama, s, o)
  File "/home/cike/marl_collector/adhoc_dt/Trainer.py", line 36, in MIE_loss
    mu1, log_var1 = teammateencoder(s)
  File "/home/cike/anaconda3/envs/pymarl2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cike/marl_collector/adhoc_dt/Networks/TeammateEncoder.py", line 26, in forward
    attn_output, _ = self.self_attention(embedded_states, embedded_states, embedded_states)
  File "/home/cike/anaconda3/envs/pymarl2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cike/anaconda3/envs/pymarl2/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 1174, in forward
    attn_mask=attn_mask, average_attn_weights=average_attn_weights)
  File "/home/cike/anaconda3/envs/pymarl2/lib/python3.7/site-packages/torch/nn/functional.py", line 5161, in multi_head_attention_forward
    attn_output_weights = softmax(attn_output_weights, dim=-1)
  File "/home/cike/anaconda3/envs/pymarl2/lib/python3.7/site-packages/torch/nn/functional.py", line 1841, in softmax
    ret = input.softmax(dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 47.46 GiB total capacity; 4.04 GiB already allocated; 3.99 GiB free; 4.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
